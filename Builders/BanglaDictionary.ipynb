{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BanglaDictionary.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uZF45UCgPKx5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set up environment (donwload and import libs)"
      ],
      "metadata": {
        "id": "uZF45UCgPKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.16.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "Kck-P8Ez0S3O",
        "outputId": "11ca33ae-f3d2-43df-d79a-0f33a8ede843"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.16.1\n",
            "  Downloading numpy-1.16.1-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 595 kB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.16.1 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.16.1 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.16.1 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.1 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.1 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.16.1 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.1 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.1 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.16.1 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.1 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.16.1 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.16.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dALtw7m-O2st",
        "outputId": "bd77e2ae-4b3c-4ed8-e3ee-54d3f988de35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-19 11:34:15--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2022-05-19 11:34:15--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2022-05-19 11:34:16--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1603 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-19 11:34:16 (18.8 MB/s) - written to stdout [1603/1603]\n",
            "\n",
            "setup Colab for PySpark 3.0.3 and Spark NLP 3.4.4\n",
            "Installing PySpark 3.0.3 and Spark NLP 3.4.4\n",
            "\u001b[K     |████████████████████████████████| 209.1 MB 55 kB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 34.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 52.5 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# SPARK-NLP FOR TOKENIZATION, LEMATIZATION, PoS\n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alphabet-detector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ija9-xRBPXsf",
        "outputId": "7fe22592-b4f3-4773-a12c-d9dad3addb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alphabet-detector\n",
            "  Downloading alphabet-detector-0.0.7.tar.gz (1.6 kB)\n",
            "Building wheels for collected packages: alphabet-detector\n",
            "  Building wheel for alphabet-detector (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alphabet-detector: filename=alphabet_detector-0.0.7-py3-none-any.whl size=2446 sha256=cd2052d9652fe88426004cca4917c7f27ed36a3890eb0a8ccb38af14f6aa00fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/8c/ab/4afb1765f2b8450f894a1f06c9aa2b3f8e73f2fb8b55849e17\n",
            "Successfully built alphabet-detector\n",
            "Installing collected packages: alphabet-detector\n",
            "Successfully installed alphabet-detector-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from alphabet_detector import AlphabetDetector"
      ],
      "metadata": {
        "id": "ugxjJT3LPaPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start()\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aczty4_iQkVL",
        "outputId": "ac47de07-2847-4dc8-c414-cfa05321631a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version: 3.4.4\n",
            "Apache Spark version: 3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start()\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwT6JEJ_QnFs",
        "outputId": "d0367ab2-d717-46d9-f0fc-d81f6ebc7b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version: 3.4.4\n",
            "Apache Spark version: 3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained(\"lemma\", \"bn\") \\\n",
        "        .setInputCols([\"token\"]) \\\n",
        "        .setOutputCol(\"lemma\")\n",
        "\n",
        "stop_words = StopWordsCleaner.pretrained('stopwords_bn', 'bn')\\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"cleanTokens\") \\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "pos = PerceptronModel.pretrained(\"pos_msri\", \"bn\") \\\n",
        "  .setInputCols([\"document\", \"token\"]) \\\n",
        "  .setOutputCol(\"pos\")\n",
        "\n",
        "nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer, lemmatizer, stop_words, pos])\n",
        "light_pipeline = LightPipeline(nlp_pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlitOQemTQ7Q",
        "outputId": "f2c0a021-bb31-41e3-f06a-212346c8830d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma download started this may take some time.\n",
            "Approximate size to download 90.6 KB\n",
            "[OK!]\n",
            "stopwords_bn download started this may take some time.\n",
            "Approximate size to download 1.9 KB\n",
            "[OK!]\n",
            "pos_msri download started this may take some time.\n",
            "Approximate size to download 806.5 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text processing (explanation)"
      ],
      "metadata": {
        "id": "RHamDDSJSaty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main aim of preprocessing is clearing text from:\n",
        "* stop-words\n",
        "* auxiliary part of speech\n",
        "* proper nouns (I am changing them to word \"জিনিস\" - \"object\")\n",
        "* numbers (I am changing them to word \"সংখ্যা\" - \"number\")\n",
        "* punctuation and other symbols"
      ],
      "metadata": {
        "id": "us86MgstShhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am also changing all pronouns to the word \"জিনিস\" - \"object\". Below listed all part of speeches that model can distinguish and what I'm doing with them in text preprocessing:"
      ],
      "metadata": {
        "id": "kHh3mNvzV-1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NN - noun \\\\\n",
        "- SYM - symbol (delete) \\\\\n",
        "- NNP - propper noun (change to \"object\") \\\\\n",
        "- VM - modal verb \\\\\n",
        "- INTF - intesifier (delete) \\\\\n",
        "- JJ - Adjective \\\\\n",
        "- QF - Quantifiers (delete) \\\\\n",
        "- CC - coordinating conjunction (delete) \\\\\n",
        "- NST - noun \\\\\n",
        "- PSP - adposition (delete) \\\\\n",
        "- DEM - pronoun (change to \"object\") \\\\\n",
        "- PRP - posessive pronoun (change to \"object\") \\\\\n",
        "- NEG - negative (delete) \\\\\n",
        "- WQ - wh-qual (delete) \\\\\n",
        "- RB - adverb \\\\\n",
        "- VAUX - Verb Auxiliary (delete) \\\\\n",
        "- UT (delete) \\\\\n",
        "- XC (delete) \\\\\n",
        "- RP - particle (delete) \\\\\n",
        "- Q0 - ordinal number (change to \"number\") \\\\\n",
        "- QC - cardinal number (change to \"number\") \\\\\n",
        "- BM - (delete) \\\\\n",
        "- NNC - compound noun \\\\\n",
        "- PPR - postposition (delete) \\\\\n",
        "- INJ - delete \\\\\n",
        "- CL - delete \\\\\n",
        "- UNK - delete \\\\"
      ],
      "metadata": {
        "id": "sckhsiQ4WxMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words i'm clearing form text are listed here \\\\\n",
        "(https://github.com/stopwords-iso/stopwords-bn/blob/master/stopwords-bn.txt)"
      ],
      "metadata": {
        "id": "wd1uioXKXmMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the words of other parts of speech, I highlight the initial form. \n",
        "As i understood from wikipedia, some part of speech in bengali has different forms of word (like nominative, objective, genetive, locative noun inflections). As a result, I get a text containing the initial forms of words and cleaned during preprocessing. Here is an example of how it should work for english language:"
      ],
      "metadata": {
        "id": "VG3hb2mFYG-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In:** She jumped into the river and breathed heavily. \\\\\n",
        "**Out:** Object jump river breath heavily "
      ],
      "metadata": {
        "id": "gN2ZtWVVZWP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual code for text processer and several examples:"
      ],
      "metadata": {
        "id": "3eEkP4RUacm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/stopwords-iso/stopwords-bn/blob/master/stopwords-bn.txt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhAxLotQaj_e",
        "outputId": "df782e77-7aa3-4399-920c-232b469d03a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-19 11:36:42--  https://github.com/stopwords-iso/stopwords-bn/blob/master/stopwords-bn.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘stopwords-bn.txt’\n",
            "\n",
            "stopwords-bn.txt        [ <=>                ] 218.26K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-19 11:36:42 (2.10 MB/s) - ‘stopwords-bn.txt’ saved [223502]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stopwords-bn.txt', 'r', encoding='utf-8') as stopfile:\n",
        "  text = stopfile.read()\n",
        "  stopwords = text.split('\\n')\n",
        "print('Web stop_words:', len(stopwords))\n",
        "stop_set = set(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hf3EZ4qbFEv",
        "outputId": "48649952-4373-45e2-d981-be174883f939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Web stop_words: 3014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "banned_types = {'SYM', 'INTF', 'QF', 'CC', 'PSP', 'RDP', 'NEG', 'WQ', 'VAUX', 'UT', 'XC', 'RP', 'BM', 'PPR', 'INJ', 'CL', 'UNK'}\n",
        "pron_types = {'NNP', 'DEM', 'PRP'}\n",
        "number_types = {'QO', 'QC'}\n",
        "forbidden_chars = {'\"','0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0','!', '@', '#', '$', '^', '&', '*', '—','’','‘',']','[','_','·','.%',')',\n",
        "'(','”','“','\\u3000',\"、\",\"。\",\"〈\",\"〉\",\"《\"}"
      ],
      "metadata": {
        "id": "zYJntxGqb5L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_bn_text(text_in, debug=False):\n",
        "  ad = AlphabetDetector()\n",
        "  results = light_pipeline.fullAnnotate([text_in])\n",
        "  text_out = \"\"\n",
        "\n",
        "  for i in range(len(results[0]['lemma'])):\n",
        "    token = results[0]['token'][i].result\n",
        "    lemm = results[0]['lemma'][i].result\n",
        "    pt = results[0]['pos'][i].result\n",
        "\n",
        "    if not \"LATIN\" in ad.detect_alphabet(token) and token not in stop_set and lemm not in stop_set and pt not in banned_types and len(set(token).intersection(forbidden_chars)) == 0:\n",
        "      if pt in pron_types:\n",
        "        text_out += 'জিনিস '\n",
        "        if debug:\n",
        "          print(token, \"-->\", \"জিনিস\", pt)\n",
        "      elif pt in number_types:\n",
        "        text_out += 'সংখ্যা '\n",
        "        if debug:\n",
        "          print(token, \"-->\", \"সংখ্যা\", pt)\n",
        "      else:\n",
        "        lemm = re.sub(r'[^\\w\\s]', '', lemm)\n",
        "        text_out += lemm\n",
        "        text_out += ' '\n",
        "        if debug:\n",
        "          print(token, \"-->\", lemm, pt)\n",
        "    else:\n",
        "      if debug:\n",
        "        print(token, \"-->\", \"[deleted]\", pt)\n",
        "  return text_out"
      ],
      "metadata": {
        "id": "HFr49lfpcEW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run cells below for testing"
      ],
      "metadata": {
        "id": "Ypt-28Mvevuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences to test preprocesser. You can put sentence to test in this list\n",
        "texts = [\n",
        "  'ধারা ১: সমস্ত মানুষ স্বাধীনভাবে সমান মর্যাদা এবং অধিকার নিয়ে জন্মগ্রহণ করে। তাঁদের বিবেক এবং বুদ্ধি আছে; সুতরাং সকলেরই একে অপরের প্রতি ভ্রাতৃত্বসুলভ মনোভাব নিয়ে আচরণ করা উচিৎ।',\n",
        "  'একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও',\n",
        "  'তিনি জানালা খোলা এবং দেখেছি একটি গাছ পাখি.'\n",
        "]"
      ],
      "metadata": {
        "id": "j1WJXZoybhzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for text in texts:\n",
        "  print(\"Test\", i)\n",
        "  i += 1\n",
        "\n",
        "  print('\\033[1m' + \"In:\" +  '\\033[0m', text)\n",
        "  print('\\033[1m' + \"Out:\" +  '\\033[0m', clean_bn_text(text, True)) # Can set False instead of True to get rid off \"A --> B\" debug output \n",
        "  print(\"==========================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDEJ-S-4epCa",
        "outputId": "a0334a2d-e335-47b3-d180-39c361274fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1\n",
            "\u001b[1mIn:\u001b[0m ধারা ১: সমস্ত মানুষ স্বাধীনভাবে সমান মর্যাদা এবং অধিকার নিয়ে জন্মগ্রহণ করে। তাঁদের বিবেক এবং বুদ্ধি আছে; সুতরাং সকলেরই একে অপরের প্রতি ভ্রাতৃত্বসুলভ মনোভাব নিয়ে আচরণ করা উচিৎ।\n",
            "ধারা --> ধর NN\n",
            "১ --> সংখ্যা QC\n",
            ": --> [deleted] SYM\n",
            "সমস্ত --> সমসত JJ\n",
            "মানুষ --> মনষ NN\n",
            "স্বাধীনভাবে --> সবধনভব RB\n",
            "সমান --> সমন JJ\n",
            "মর্যাদা --> মরযদ NN\n",
            "এবং --> [deleted] CC\n",
            "অধিকার --> অধকর NN\n",
            "নিয়ে --> নয JJ\n",
            "জন্মগ্রহণ --> জনমগরহণ NN\n",
            "করে। --> কর NN\n",
            "তাঁদের --> জিনিস PRP\n",
            "বিবেক --> ববক JJ\n",
            "এবং --> [deleted] CC\n",
            "বুদ্ধি --> বদধ NN\n",
            "আছে --> আছ VM\n",
            "; --> [deleted] SYM\n",
            "সুতরাং --> সতর JJ\n",
            "সকলেরই --> সকলরই NN\n",
            "একে --> সংখ্যা QC\n",
            "অপরের --> অপরর NN\n",
            "প্রতি --> [deleted] PSP\n",
            "ভ্রাতৃত্বসুলভ --> ভরততবসলভ NN\n",
            "মনোভাব --> মনভব NN\n",
            "নিয়ে --> নয JJ\n",
            "আচরণ --> আচরণ NN\n",
            "করা --> কর VM\n",
            "উচিৎ। --> [deleted] VAUX\n",
            "\u001b[1mOut:\u001b[0m ধর সংখ্যা সমসত মনষ সবধনভব সমন মরযদ অধকর নয জনমগরহণ কর জিনিস ববক বদধ আছ সতর সকলরই সংখ্যা অপরর ভরততবসলভ মনভব নয আচরণ কর \n",
            "==========================================================\n",
            "Test 2\n",
            "\u001b[1mIn:\u001b[0m একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও\n",
            "একদিন --> একদন NN\n",
            "প্রাতে --> পরত NN\n",
            "বৈদ্যনাথের --> বদযনথ NN\n",
            "মার্বলমণ্ডিত --> মরবলমণডত NN\n",
            "দালানে --> দলন NN\n",
            "একটি --> সংখ্যা QC\n",
            "স্থূলোদর --> সথলউদর JJ\n",
            "সন্ন্যাসী --> সননযস NN\n",
            "দুইসের --> দইসর NN\n",
            "মোহনভোগ --> মহনভগ NN\n",
            "এবং --> [deleted] CC\n",
            "দেড়সের --> দডসর NN\n",
            "দুগ্ধ --> দগধ NN\n",
            "সেবায় --> সব NN\n",
            "নিযুক্ত --> নযকত JJ\n",
            "আছে --> আছ VM\n",
            "বৈদ্যনাথ --> [deleted] XC\n",
            "গায়ে --> গ NN\n",
            "একখানি --> সংখ্যা QC\n",
            "চাদর --> চদর NN\n",
            "দিয়া --> [deleted] PSP\n",
            "জোড়করে --> জডকর NN\n",
            "একান্ত --> একনত RB\n",
            "বিনীতভাবে --> বনতভব RB\n",
            "ভূতলে --> ভতল NN\n",
            "বসিয়া --> বস VM\n",
            "ভক্তিভরে --> [deleted] VAUX\n",
            "পবিত্র --> পবতর JJ\n",
            "ভোজনব্যাপার --> ভজনবযপর NN\n",
            "নিরীক্ষণ --> নরকষণ NN\n",
            "করিতেছিলেন --> কর VM\n",
            "এমন --> জিনিস DEM\n",
            "সময় --> সময় NN\n",
            "কোনোমতে --> কনমত NN\n",
            "দ্বারীদের --> দবর NN\n",
            "দৃষ্টি --> দষট NN\n",
            "এড়াইয়া --> জিনিস PRP\n",
            "জীর্ণদেহ --> জরণদহ NN\n",
            "বালক --> বলক NN\n",
            "সহিত --> সহত NN\n",
            "একটি --> সংখ্যা QC\n",
            "অতি --> [deleted] INTF\n",
            "শীর্ণকায়া --> শরণকয় JJ\n",
            "রমণী --> রমণ NN\n",
            "গৃহে --> গহ NN\n",
            "প্রবেশ --> পরবশ NN\n",
            "করিয়া --> বশবস VM\n",
            "ক্ষীণস্বরে --> কষণসবর NN\n",
            "কহিল --> কহ JJ\n",
            "বাবু --> বব NN\n",
            "দুটি --> সংখ্যা QC\n",
            "খেতে --> খওয় NN\n",
            "দাও --> [deleted] VAUX\n",
            "\u001b[1mOut:\u001b[0m একদন পরত বদযনথ মরবলমণডত দলন সংখ্যা সথলউদর সননযস দইসর মহনভগ দডসর দগধ সব নযকত আছ গ সংখ্যা চদর জডকর একনত বনতভব ভতল বস পবতর ভজনবযপর নরকষণ কর জিনিস সময় কনমত দবর দষট জিনিস জরণদহ বলক সহত সংখ্যা শরণকয় রমণ গহ পরবশ বশবস কষণসবর কহ বব সংখ্যা খওয় \n",
            "==========================================================\n",
            "Test 3\n",
            "\u001b[1mIn:\u001b[0m তিনি জানালা খোলা এবং দেখেছি একটি গাছ পাখি.\n",
            "তিনি --> জিনিস PRP\n",
            "জানালা --> জনল NN\n",
            "খোলা --> খল VM\n",
            "এবং --> [deleted] CC\n",
            "দেখেছি --> সংখ্যা QC\n",
            "একটি --> সংখ্যা QC\n",
            "গাছ --> গছ NN\n",
            "পাখি --> পখ NN\n",
            ". --> [deleted] SYM\n",
            "\u001b[1mOut:\u001b[0m জিনিস জনল খল সংখ্যা সংখ্যা গছ পখ \n",
            "==========================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "корректность лемматизатора была проверена (sreenjay07@gmail.com)."
      ],
      "metadata": {
        "id": "-L4nT9r_t8Ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Построение словаря\n"
      ],
      "metadata": {
        "id": "8esvXLsLxSpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import svds\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "IfermAboxWTD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Строю TF-IDF\n",
        "def make_table_and_dict(corpus_path, min_df, max_df):\n",
        "    with open(corpus_path, 'r', encoding = 'utf-8') as corpus_file:\n",
        "        vectorizer = TfidfVectorizer(analyzer = 'word', min_df = min_df, max_df = max_df)\n",
        "        data_vectorized = vectorizer.fit_transform(corpus_file)\n",
        "    return data_vectorized, vectorizer.get_feature_names_out(), vectorizer.idf_\n",
        "\n",
        "# Строю словарь\n",
        "def create_table(data_vectorized, k, table_path, name):\n",
        "    u, sigma, vt = svds(data_vectorized, k)\n",
        "\n",
        "    # Переупорядочиваем, чтобы ветора соответствовали сингулярным значениям в убывающем порядке\n",
        "    u = u[:, ::-1]\n",
        "    sigma = np.diag(sigma)[::-1, ::-1]\n",
        "    vt = vt[::-1]\n",
        "\n",
        "    with open(table_path + name + str(k) + '.npy', 'wb') as f:\n",
        "        np.save(f, np.dot(sigma, vt).T)"
      ],
      "metadata": {
        "id": "DCCZDH0hxpKF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQSFVwBmz07D",
        "outputId": "5df2d342-b55c-40e1-a417-9d09f4935e0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn_data_vectorized, bn_dictionary, idfs = make_table_and_dict('/content/gdrive/My Drive/bn_corpus_cleaned.txt', 0.01, 0.99)\n",
        "with open('bn_words.npy', 'wb') as f:\n",
        "  np.save(f, bn_dictionary)\n",
        "with open('bn_data_vectorized.bn', 'wb') as f:\n",
        "  np.save(f, bn_data_vectorized)\n",
        "\n",
        "create_table(bn_data_vectorized, 1024, '/content/gdrive/My Drive/', \"bn_vectors_\") "
      ],
      "metadata": {
        "id": "0sEYn_Mjy55N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получаю словарь, представляемый в виде двух файлов: bn_words.npy это столбец уникальных лексем словаря, а bn_vectors_k.npy соответствуемый слову вектор. Ниже функция, дающая классический питоновский словарь из такого представления"
      ],
      "metadata": {
        "id": "CMQVFI9QSDKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "words = np.load('/content/gdrive/My Drive/bn_words.npy', allow_pickle=True)\n",
        "vectors = np.load('/content/gdrive/My Drive/bn_vectors_1024.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "7OlBRmQWzj_w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dict(key_word, vectors):\n",
        "  zipper = zip(key_word, vectors)\n",
        "  return dict(zipper)\n",
        "\n",
        "bn_dictionary = build_dict(words, vectors)"
      ],
      "metadata": {
        "id": "3-kDHDuY0hVS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn_dictionary['হয়ছ']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL8dwzxu08LY",
        "outputId": "b696e707-f520-4d3b-ef85-81434d105d13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.61084703,  0.25970415,  0.45921452, ...,  0.01188649,\n",
              "       -0.00665816,  0.00101615])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}